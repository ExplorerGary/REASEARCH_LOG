# 2025年6月30日

## 明确的任务：
1. 你可以在整个跑完前，应该可以先看一下某个层的参数的相关通信数据的统计，比如最前面的几层，以及最后面的几层，看看1）是否符合GGD，2）网络不同深度的统计是不是uniform的，还是根据层数有比较明显不同的统计特征

2. 把东西存在一个巨大的文件里，然后random access，只要知道offset就行了。
    警告：这可能涉及到CPU-GPU的通信，因此可能会比较麻烦，训练的效率有可能打折扣。我需要想一想或者跟别人交流下



## 今天要干的事情：
1. 使用一个MINST模型，学习使用HF Trainer，并添加钩子 【完成】
2. 将这个方式迁移到大模型上去。 【完成】

## 今日汇报：
汇报一下：

HF Trainer的钩子已经挂上了，已经可以保存文件。

    但是我还得去研究下如何把他们保存在一个巨大的文件里方便日后random access

目前使用的是Qwen2.5-7B对FreedomIntelligence/medical-o1-reasoning-SFT/zh进行微调，以期训练一个“会讲中文的医疗特化垂直LLM”

    训练的流程跑通了，但是模型的表现还有待评估



通信数据报告：
---
idx [0-141，共142个bucket一个batch]
rank [0/1] -- 目前还是两张卡训练
buffer [一个巨大的单维向量] -- 也就是各个module的grad
dtype [fp16]
一次通信的数据 [约29GB(两张GPU为记)] -- 有点大，我得把minimind的数据下载下来，给Qwen的数据腾位置了
---
桶里装了啥(具体的名称) [TODO]


通信数据本身：

是否符合GGD：应该是

从一个batch中拉取的统计数据：
    count    284.000000
    mean       0.867141
    std        0.330519
    min        0.160323
    25%        0.663619
    50%        0.857262
    75%        1.067788
    max        1.987978

直方图如下：






