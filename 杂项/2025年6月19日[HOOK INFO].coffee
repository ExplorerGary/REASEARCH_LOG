[HOOK INFO]
Batch: Batch_2, Communication: Communication_3_bucket_1, Rank: rank_0
Bucket index: 1
Flat tensor shape: torch.Size([7079936])
Buffer size: 28319744 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.7.mlp.up_proj.weight
  - module.model.layers.7.mlp.gate_proj.weight
  - module.model.layers.7.post_attention_layernorm.weight
  - module.model.layers.7.self_attn.o_proj.weight
  - module.model.layers.7.self_attn.v_proj.weight
  - module.model.layers.7.self_attn.k_proj.weight
  - module.model.layers.7.self_attn.q_proj.weight
  - module.model.layers.7.input_layernorm.weight
  - module.model.layers.6.mlp.down_proj.weight
  - module.model.layers.6.mlp.up_proj.weight
  - module.model.layers.6.mlp.gate_proj.weight
  - module.model.layers.6.post_attention_layernorm.weight
  - module.model.layers.6.self_attn.o_proj.weight
  - module.model.layers.6.self_attn.v_proj.weight
  - module.model.layers.6.self_attn.k_proj.weight
  - module.model.layers.6.self_attn.q_proj.weight
  - module.model.layers.6.input_layernorm.weight
  - module.model.layers.5.mlp.down_proj.weight
  - module.model.layers.5.mlp.up_proj.weight
  - module.model.layers.5.mlp.gate_proj.weight

[HOOK INFO]
Batch: Batch_2, Communication: Communication_3_bucket_1, Rank: rank_1
Bucket index: 1
Flat tensor shape: torch.Size([7079936])
Buffer size: 28319744 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.7.mlp.up_proj.weight
  - module.model.layers.7.mlp.gate_proj.weight
  - module.model.layers.7.post_attention_layernorm.weight
  - module.model.layers.7.self_attn.o_proj.weight
  - module.model.layers.7.self_attn.v_proj.weight
  - module.model.layers.7.self_attn.k_proj.weight
  - module.model.layers.7.self_attn.q_proj.weight
  - module.model.layers.7.input_layernorm.weight
  - module.model.layers.6.mlp.down_proj.weight
  - module.model.layers.6.mlp.up_proj.weight
  - module.model.layers.6.mlp.gate_proj.weight
  - module.model.layers.6.post_attention_layernorm.weight
  - module.model.layers.6.self_attn.o_proj.weight
  - module.model.layers.6.self_attn.v_proj.weight
  - module.model.layers.6.self_attn.k_proj.weight
  - module.model.layers.6.self_attn.q_proj.weight
  - module.model.layers.6.input_layernorm.weight
  - module.model.layers.5.mlp.down_proj.weight
  - module.model.layers.5.mlp.up_proj.weight
  - module.model.layers.5.mlp.gate_proj.weight

[HOOK INFO]
Batch: Batch_2, Communication: Communication_4_bucket_2, Rank: rank_0
Bucket index: 2
Flat tensor shape: torch.Size([7015424])
Buffer size: 28061696 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.5.post_attention_layernorm.weight
  - module.model.layers.5.self_attn.o_proj.weight
  - module.model.layers.5.self_attn.v_proj.weight
  - module.model.layers.5.self_attn.k_proj.weight
  - module.model.layers.5.self_attn.q_proj.weight
  - module.model.layers.5.input_layernorm.weight
  - module.model.layers.4.mlp.down_proj.weight
  - module.model.layers.4.mlp.up_proj.weight
  - module.model.layers.4.mlp.gate_proj.weight
  - module.model.layers.4.post_attention_layernorm.weight
  - module.model.layers.4.self_attn.o_proj.weight
  - module.model.layers.4.self_attn.v_proj.weight
  - module.model.layers.4.self_attn.k_proj.weight
  - module.model.layers.4.self_attn.q_proj.weight
  - module.model.layers.4.input_layernorm.weight
  - module.model.layers.3.mlp.down_proj.weight
  - module.model.layers.3.mlp.up_proj.weight
  - module.model.layers.3.mlp.gate_proj.weight
  - module.model.layers.3.post_attention_layernorm.weight
  - module.model.layers.3.self_attn.o_proj.weight
  - module.model.layers.3.self_attn.v_proj.weight
  - module.model.layers.3.self_attn.k_proj.weight
  - module.model.layers.3.self_attn.q_proj.weight
  - module.model.layers.3.input_layernorm.weight
  - module.model.layers.2.mlp.down_proj.weight

[HOOK INFO]
Batch: Batch_2, Communication: Communication_4_bucket_2, Rank: rank_1
Bucket index: 2
Flat tensor shape: torch.Size([7015424])
Buffer size: 28061696 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.5.post_attention_layernorm.weight
  - module.model.layers.5.self_attn.o_proj.weight
  - module.model.layers.5.self_attn.v_proj.weight
  - module.model.layers.5.self_attn.k_proj.weight
  - module.model.layers.5.self_attn.q_proj.weight
  - module.model.layers.5.input_layernorm.weight
  - module.model.layers.4.mlp.down_proj.weight
  - module.model.layers.4.mlp.up_proj.weight
  - module.model.layers.4.mlp.gate_proj.weight
  - module.model.layers.4.post_attention_layernorm.weight
  - module.model.layers.4.self_attn.o_proj.weight
  - module.model.layers.4.self_attn.v_proj.weight
  - module.model.layers.4.self_attn.k_proj.weight
  - module.model.layers.4.self_attn.q_proj.weight
  - module.model.layers.4.input_layernorm.weight
  - module.model.layers.3.mlp.down_proj.weight
  - module.model.layers.3.mlp.up_proj.weight
  - module.model.layers.3.mlp.gate_proj.weight
  - module.model.layers.3.post_attention_layernorm.weight
  - module.model.layers.3.self_attn.o_proj.weight
  - module.model.layers.3.self_attn.v_proj.weight
  - module.model.layers.3.self_attn.k_proj.weight
  - module.model.layers.3.self_attn.q_proj.weight
  - module.model.layers.3.input_layernorm.weight
  - module.model.layers.2.mlp.down_proj.weight

[HOOK INFO]
Batch: Batch_2, Communication: Communication_5_bucket_3, Rank: rank_0
Bucket index: 3
[HOOK INFO]Flat tensor shape: torch.Size([7079936])

Batch: Batch_2, Communication: Communication_5_bucket_3, Rank: rank_1Buffer size: 28319744 bytes

Bucket index: 3
Data type: torch.float32
Flat tensor shape: torch.Size([7079936])
Bucket parameter names:Buffer size: 28319744 bytes

  - module.model.layers.2.mlp.up_proj.weightData type: torch.float32

  - module.model.layers.2.mlp.gate_proj.weight
  - module.model.layers.2.post_attention_layernorm.weight
  - module.model.layers.2.self_attn.o_proj.weight
  - module.model.layers.2.self_attn.v_proj.weight
  - module.model.layers.2.self_attn.k_proj.weight
  - module.model.layers.2.self_attn.q_proj.weight
Bucket parameter names:  - module.model.layers.2.input_layernorm.weight

  - module.model.layers.2.mlp.up_proj.weight  - module.model.layers.1.mlp.down_proj.weight

  - module.model.layers.2.mlp.gate_proj.weight  - module.model.layers.1.mlp.up_proj.weight

  - module.model.layers.2.post_attention_layernorm.weight  - module.model.layers.1.mlp.gate_proj.weight

  - module.model.layers.2.self_attn.o_proj.weight  - module.model.layers.1.post_attention_layernorm.weight

  - module.model.layers.2.self_attn.v_proj.weight  - module.model.layers.1.self_attn.o_proj.weight

  - module.model.layers.2.self_attn.k_proj.weight  - module.model.layers.1.self_attn.v_proj.weight

  - module.model.layers.2.self_attn.q_proj.weight  - module.model.layers.1.self_attn.k_proj.weight

  - module.model.layers.2.input_layernorm.weight  - module.model.layers.1.self_attn.q_proj.weight

  - module.model.layers.1.mlp.down_proj.weight  - module.model.layers.1.input_layernorm.weight

  - module.model.layers.1.mlp.up_proj.weight  - module.model.layers.0.mlp.down_proj.weight

  - module.model.layers.1.mlp.gate_proj.weight  - module.model.layers.0.mlp.up_proj.weight

  - module.model.layers.1.post_attention_layernorm.weight  - module.model.layers.0.mlp.gate_proj.weight

  - module.model.layers.1.self_attn.o_proj.weight

  - module.model.layers.1.self_attn.v_proj.weight
  - module.model.layers.1.self_attn.k_proj.weight
  - module.model.layers.1.self_attn.q_proj.weight
  - module.model.layers.1.input_layernorm.weight
  - module.model.layers.0.mlp.down_proj.weight
  - module.model.layers.0.mlp.up_proj.weight
  - module.model.layers.0.mlp.gate_proj.weight

[HOOK INFO]
Batch: Batch_2, Communication: Communication_6_bucket_4, Rank: rank_0
Bucket index: 4
[HOOK INFO]Flat tensor shape: torch.Size([3933184])

Buffer size: 15732736 bytes
Batch: Batch_2, Communication: Communication_6_bucket_4, Rank: rank_1Data type: torch.float32

Bucket index: 4
Bucket parameter names:
Flat tensor shape: torch.Size([3933184])  - module.model.layers.0.post_attention_layernorm.weight

Buffer size: 15732736 bytes  - module.model.layers.0.self_attn.o_proj.weight

Data type: torch.float32  - module.model.layers.0.self_attn.v_proj.weight

  - module.model.layers.0.self_attn.k_proj.weight
  - module.model.layers.0.self_attn.q_proj.weightBucket parameter names:

  - module.model.layers.0.input_layernorm.weight  - module.model.layers.0.post_attention_layernorm.weight

  - module.model.embed_tokens.weight  - module.model.layers.0.self_attn.o_proj.weight


  - module.model.layers.0.self_attn.v_proj.weight
  - module.model.layers.0.self_attn.k_proj.weight
  - module.model.layers.0.self_attn.q_proj.weight
  - module.model.layers.0.input_layernorm.weight
  - module.model.embed_tokens.weight

[HOOK INFO]
Batch: Batch_3, Communication: Communication_7_bucket_0, Rank: rank_1
Bucket index: 0
Flat tensor shape: torch.Size([721408])
Buffer size: 2885632 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.norm.weight
  - module.model.layers.7.mlp.down_proj.weight

[HOOK INFO]
Batch: Batch_3, Communication: Communication_7_bucket_0, Rank: rank_0
Bucket index: 0
Flat tensor shape: torch.Size([721408])
Buffer size: 2885632 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.norm.weight
  - module.model.layers.7.mlp.down_proj.weight

[HOOK INFO]
Batch: Batch_3, Communication: Communication_8_bucket_1, Rank: rank_1
Bucket index: 1
Flat tensor shape: torch.Size([7079936])
Buffer size: 28319744 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.7.mlp.up_proj.weight
  - module.model.layers.7.mlp.gate_proj.weight
  - module.model.layers.7.post_attention_layernorm.weight
  - module.model.layers.7.self_attn.o_proj.weight
  - module.model.layers.7.self_attn.v_proj.weight
  - module.model.layers.7.self_attn.k_proj.weight
  - module.model.layers.7.self_attn.q_proj.weight
  - module.model.layers.7.input_layernorm.weight
  - module.model.layers.6.mlp.down_proj.weight
  - module.model.layers.6.mlp.up_proj.weight
  - module.model.layers.6.mlp.gate_proj.weight
  - module.model.layers.6.post_attention_layernorm.weight
  - module.model.layers.6.self_attn.o_proj.weight
  - module.model.layers.6.self_attn.v_proj.weight
  - module.model.layers.6.self_attn.k_proj.weight
  - module.model.layers.6.self_attn.q_proj.weight
  - module.model.layers.6.input_layernorm.weight
  - module.model.layers.5.mlp.down_proj.weight
  - module.model.layers.5.mlp.up_proj.weight
  - module.model.layers.5.mlp.gate_proj.weight

[HOOK INFO]
Batch: Batch_3, Communication: Communication_8_bucket_1, Rank: rank_0
Bucket index: 1
Flat tensor shape: torch.Size([7079936])
Buffer size: 28319744 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.7.mlp.up_proj.weight
  - module.model.layers.7.mlp.gate_proj.weight
  - module.model.layers.7.post_attention_layernorm.weight
  - module.model.layers.7.self_attn.o_proj.weight
  - module.model.layers.7.self_attn.v_proj.weight
  - module.model.layers.7.self_attn.k_proj.weight
  - module.model.layers.7.self_attn.q_proj.weight
  - module.model.layers.7.input_layernorm.weight
  - module.model.layers.6.mlp.down_proj.weight
  - module.model.layers.6.mlp.up_proj.weight
  - module.model.layers.6.mlp.gate_proj.weight
  - module.model.layers.6.post_attention_layernorm.weight
  - module.model.layers.6.self_attn.o_proj.weight
  - module.model.layers.6.self_attn.v_proj.weight
  - module.model.layers.6.self_attn.k_proj.weight
  - module.model.layers.6.self_attn.q_proj.weight
  - module.model.layers.6.input_layernorm.weight
  - module.model.layers.5.mlp.down_proj.weight
  - module.model.layers.5.mlp.up_proj.weight
  - module.model.layers.5.mlp.gate_proj.weight

[HOOK INFO]
Batch: Batch_3, Communication: Communication_9_bucket_2, Rank: rank_1
Bucket index: 2
Flat tensor shape: torch.Size([7015424])
Buffer size: 28061696 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.5.post_attention_layernorm.weight
  - module.model.layers.5.self_attn.o_proj.weight
  - module.model.layers.5.self_attn.v_proj.weight
  - module.model.layers.5.self_attn.k_proj.weight
  - module.model.layers.5.self_attn.q_proj.weight
  - module.model.layers.5.input_layernorm.weight
  - module.model.layers.4.mlp.down_proj.weight
  - module.model.layers.4.mlp.up_proj.weight
  - module.model.layers.4.mlp.gate_proj.weight
  - module.model.layers.4.post_attention_layernorm.weight
  - module.model.layers.4.self_attn.o_proj.weight
  - module.model.layers.4.self_attn.v_proj.weight
  - module.model.layers.4.self_attn.k_proj.weight
  - module.model.layers.4.self_attn.q_proj.weight
  - module.model.layers.4.input_layernorm.weight
  - module.model.layers.3.mlp.down_proj.weight
  - module.model.layers.3.mlp.up_proj.weight
  - module.model.layers.3.mlp.gate_proj.weight
  - module.model.layers.3.post_attention_layernorm.weight
  - module.model.layers.3.self_attn.o_proj.weight
  - module.model.layers.3.self_attn.v_proj.weight
  - module.model.layers.3.self_attn.k_proj.weight
  - module.model.layers.3.self_attn.q_proj.weight
  - module.model.layers.3.input_layernorm.weight
  - module.model.layers.2.mlp.down_proj.weight

[HOOK INFO]
Batch: Batch_3, Communication: Communication_9_bucket_2, Rank: rank_0
Bucket index: 2
Flat tensor shape: torch.Size([7015424])
Buffer size: 28061696 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.5.post_attention_layernorm.weight
  - module.model.layers.5.self_attn.o_proj.weight
  - module.model.layers.5.self_attn.v_proj.weight
  - module.model.layers.5.self_attn.k_proj.weight
  - module.model.layers.5.self_attn.q_proj.weight
  - module.model.layers.5.input_layernorm.weight
  - module.model.layers.4.mlp.down_proj.weight
  - module.model.layers.4.mlp.up_proj.weight
  - module.model.layers.4.mlp.gate_proj.weight
  - module.model.layers.4.post_attention_layernorm.weight
  - module.model.layers.4.self_attn.o_proj.weight
  - module.model.layers.4.self_attn.v_proj.weight
  - module.model.layers.4.self_attn.k_proj.weight
  - module.model.layers.4.self_attn.q_proj.weight
  - module.model.layers.4.input_layernorm.weight
  - module.model.layers.3.mlp.down_proj.weight
  - module.model.layers.3.mlp.up_proj.weight
  - module.model.layers.3.mlp.gate_proj.weight
  - module.model.layers.3.post_attention_layernorm.weight
  - module.model.layers.3.self_attn.o_proj.weight
  - module.model.layers.3.self_attn.v_proj.weight
  - module.model.layers.3.self_attn.k_proj.weight
  - module.model.layers.3.self_attn.q_proj.weight
  - module.model.layers.3.input_layernorm.weight
  - module.model.layers.2.mlp.down_proj.weight

[HOOK INFO]
Batch: Batch_3, Communication: Communication_10_bucket_3, Rank: rank_0
[HOOK INFO]Bucket index: 3

Batch: Batch_3, Communication: Communication_10_bucket_3, Rank: rank_1
Bucket index: 3
Flat tensor shape: torch.Size([7079936])
Buffer size: 28319744 bytes
Flat tensor shape: torch.Size([7079936])Data type: torch.float32

Buffer size: 28319744 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.2.mlp.up_proj.weight
  - module.model.layers.2.mlp.gate_proj.weightBucket parameter names:

  - module.model.layers.2.post_attention_layernorm.weight  - module.model.layers.2.mlp.up_proj.weight

  - module.model.layers.2.self_attn.o_proj.weight  - module.model.layers.2.mlp.gate_proj.weight

  - module.model.layers.2.self_attn.v_proj.weight  - module.model.layers.2.post_attention_layernorm.weight

  - module.model.layers.2.self_attn.k_proj.weight  - module.model.layers.2.self_attn.o_proj.weight

  - module.model.layers.2.self_attn.q_proj.weight  - module.model.layers.2.self_attn.v_proj.weight

  - module.model.layers.2.input_layernorm.weight  - module.model.layers.2.self_attn.k_proj.weight

  - module.model.layers.1.mlp.down_proj.weight  - module.model.layers.2.self_attn.q_proj.weight

  - module.model.layers.1.mlp.up_proj.weight  - module.model.layers.2.input_layernorm.weight

  - module.model.layers.1.mlp.gate_proj.weight  - module.model.layers.1.mlp.down_proj.weight

  - module.model.layers.1.post_attention_layernorm.weight  - module.model.layers.1.mlp.up_proj.weight

  - module.model.layers.1.self_attn.o_proj.weight  - module.model.layers.1.mlp.gate_proj.weight

  - module.model.layers.1.self_attn.v_proj.weight  - module.model.layers.1.post_attention_layernorm.weight

  - module.model.layers.1.self_attn.k_proj.weight  - module.model.layers.1.self_attn.o_proj.weight

  - module.model.layers.1.self_attn.q_proj.weight  - module.model.layers.1.self_attn.v_proj.weight

  - module.model.layers.1.input_layernorm.weight  - module.model.layers.1.self_attn.k_proj.weight

  - module.model.layers.0.mlp.down_proj.weight  - module.model.layers.1.self_attn.q_proj.weight

  - module.model.layers.0.mlp.up_proj.weight  - module.model.layers.1.input_layernorm.weight

  - module.model.layers.0.mlp.gate_proj.weight  - module.model.layers.0.mlp.down_proj.weight


  - module.model.layers.0.mlp.up_proj.weight
  - module.model.layers.0.mlp.gate_proj.weight

[HOOK INFO]
Batch: Batch_3, Communication: Communication_11_bucket_4, Rank: rank_0
Bucket index: 4[HOOK INFO]

Batch: Batch_3, Communication: Communication_11_bucket_4, Rank: rank_1
Bucket index: 4Flat tensor shape: torch.Size([3933184])

Buffer size: 15732736 bytes
Data type: torch.float32Flat tensor shape: torch.Size([3933184])

Buffer size: 15732736 bytesBucket parameter names:

Data type: torch.float32  - module.model.layers.0.post_attention_layernorm.weight

  - module.model.layers.0.self_attn.o_proj.weight
  - module.model.layers.0.self_attn.v_proj.weightBucket parameter names:

  - module.model.layers.0.self_attn.k_proj.weight  - module.model.layers.0.post_attention_layernorm.weight

  - module.model.layers.0.self_attn.q_proj.weight  - module.model.layers.0.self_attn.o_proj.weight

  - module.model.layers.0.input_layernorm.weight  - module.model.layers.0.self_attn.v_proj.weight

  - module.model.embed_tokens.weight  - module.model.layers.0.self_attn.k_proj.weight


  - module.model.layers.0.self_attn.q_proj.weight
  - module.model.layers.0.input_layernorm.weight
  - module.model.embed_tokens.weight

[HOOK INFO]
Batch: Batch_4, Communication: Communication_12_bucket_0, Rank: rank_1
Bucket index: 0
Flat tensor shape: torch.Size([721408])
Buffer size: 2885632 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.norm.weight
  - module.model.layers.7.mlp.down_proj.weight

[HOOK INFO]
Batch: Batch_4, Communication: Communication_12_bucket_0, Rank: rank_0
Bucket index: 0
Flat tensor shape: torch.Size([721408])
Buffer size: 2885632 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.norm.weight
  - module.model.layers.7.mlp.down_proj.weight

[HOOK INFO]
Batch: Batch_4, Communication: Communication_13_bucket_1, Rank: rank_1
Bucket index: 1
Flat tensor shape: torch.Size([7079936])
Buffer size: 28319744 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.7.mlp.up_proj.weight
  - module.model.layers.7.mlp.gate_proj.weight
  - module.model.layers.7.post_attention_layernorm.weight
  - module.model.layers.7.self_attn.o_proj.weight
  - module.model.layers.7.self_attn.v_proj.weight
  - module.model.layers.7.self_attn.k_proj.weight
  - module.model.layers.7.self_attn.q_proj.weight
  - module.model.layers.7.input_layernorm.weight
  - module.model.layers.6.mlp.down_proj.weight
  - module.model.layers.6.mlp.up_proj.weight
  - module.model.layers.6.mlp.gate_proj.weight
  - module.model.layers.6.post_attention_layernorm.weight
  - module.model.layers.6.self_attn.o_proj.weight
  - module.model.layers.6.self_attn.v_proj.weight
  - module.model.layers.6.self_attn.k_proj.weight
  - module.model.layers.6.self_attn.q_proj.weight
  - module.model.layers.6.input_layernorm.weight
  - module.model.layers.5.mlp.down_proj.weight
  - module.model.layers.5.mlp.up_proj.weight
  - module.model.layers.5.mlp.gate_proj.weight

[HOOK INFO]
Batch: Batch_4, Communication: Communication_13_bucket_1, Rank: rank_0
Bucket index: 1
Flat tensor shape: torch.Size([7079936])
Buffer size: 28319744 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.7.mlp.up_proj.weight
  - module.model.layers.7.mlp.gate_proj.weight
  - module.model.layers.7.post_attention_layernorm.weight
  - module.model.layers.7.self_attn.o_proj.weight
  - module.model.layers.7.self_attn.v_proj.weight
  - module.model.layers.7.self_attn.k_proj.weight
  - module.model.layers.7.self_attn.q_proj.weight
  - module.model.layers.7.input_layernorm.weight
  - module.model.layers.6.mlp.down_proj.weight
  - module.model.layers.6.mlp.up_proj.weight
  - module.model.layers.6.mlp.gate_proj.weight
  - module.model.layers.6.post_attention_layernorm.weight
  - module.model.layers.6.self_attn.o_proj.weight
  - module.model.layers.6.self_attn.v_proj.weight
  - module.model.layers.6.self_attn.k_proj.weight
  - module.model.layers.6.self_attn.q_proj.weight
  - module.model.layers.6.input_layernorm.weight
  - module.model.layers.5.mlp.down_proj.weight
  - module.model.layers.5.mlp.up_proj.weight
  - module.model.layers.5.mlp.gate_proj.weight

[HOOK INFO]
Batch: Batch_4, Communication: Communication_14_bucket_2, Rank: rank_1
Bucket index: 2
Flat tensor shape: torch.Size([7015424])
Buffer size: 28061696 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.5.post_attention_layernorm.weight
  - module.model.layers.5.self_attn.o_proj.weight
  - module.model.layers.5.self_attn.v_proj.weight
  - module.model.layers.5.self_attn.k_proj.weight
  - module.model.layers.5.self_attn.q_proj.weight
  - module.model.layers.5.input_layernorm.weight
  - module.model.layers.4.mlp.down_proj.weight
  - module.model.layers.4.mlp.up_proj.weight
  - module.model.layers.4.mlp.gate_proj.weight
  - module.model.layers.4.post_attention_layernorm.weight
  - module.model.layers.4.self_attn.o_proj.weight
  - module.model.layers.4.self_attn.v_proj.weight
  - module.model.layers.4.self_attn.k_proj.weight
  - module.model.layers.4.self_attn.q_proj.weight
  - module.model.layers.4.input_layernorm.weight
  - module.model.layers.3.mlp.down_proj.weight
  - module.model.layers.3.mlp.up_proj.weight
  - module.model.layers.3.mlp.gate_proj.weight
  - module.model.layers.3.post_attention_layernorm.weight
  - module.model.layers.3.self_attn.o_proj.weight
  - module.model.layers.3.self_attn.v_proj.weight
  - module.model.layers.3.self_attn.k_proj.weight
  - module.model.layers.3.self_attn.q_proj.weight
  - module.model.layers.3.input_layernorm.weight
  - module.model.layers.2.mlp.down_proj.weight

[HOOK INFO]
Batch: Batch_4, Communication: Communication_14_bucket_2, Rank: rank_0
Bucket index: 2
Flat tensor shape: torch.Size([7015424])
Buffer size: 28061696 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.5.post_attention_layernorm.weight
  - module.model.layers.5.self_attn.o_proj.weight
  - module.model.layers.5.self_attn.v_proj.weight
  - module.model.layers.5.self_attn.k_proj.weight
  - module.model.layers.5.self_attn.q_proj.weight
  - module.model.layers.5.input_layernorm.weight
  - module.model.layers.4.mlp.down_proj.weight
  - module.model.layers.4.mlp.up_proj.weight
  - module.model.layers.4.mlp.gate_proj.weight
  - module.model.layers.4.post_attention_layernorm.weight
  - module.model.layers.4.self_attn.o_proj.weight
  - module.model.layers.4.self_attn.v_proj.weight
  - module.model.layers.4.self_attn.k_proj.weight
  - module.model.layers.4.self_attn.q_proj.weight
  - module.model.layers.4.input_layernorm.weight
  - module.model.layers.3.mlp.down_proj.weight
  - module.model.layers.3.mlp.up_proj.weight
  - module.model.layers.3.mlp.gate_proj.weight
  - module.model.layers.3.post_attention_layernorm.weight
  - module.model.layers.3.self_attn.o_proj.weight
  - module.model.layers.3.self_attn.v_proj.weight
  - module.model.layers.3.self_attn.k_proj.weight
  - module.model.layers.3.self_attn.q_proj.weight
  - module.model.layers.3.input_layernorm.weight
  - module.model.layers.2.mlp.down_proj.weight

[HOOK INFO]
Batch: Batch_4, Communication: Communication_15_bucket_3, Rank: rank_0
Bucket index: 3
Flat tensor shape: torch.Size([7079936])
Buffer size: 28319744 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.2.mlp.up_proj.weight
  - module.model.layers.2.mlp.gate_proj.weight
  - module.model.layers.2.post_attention_layernorm.weight
[HOOK INFO]  - module.model.layers.2.self_attn.o_proj.weight

Batch: Batch_4, Communication: Communication_15_bucket_3, Rank: rank_1  - module.model.layers.2.self_attn.v_proj.weight

Bucket index: 3  - module.model.layers.2.self_attn.k_proj.weight

  - module.model.layers.2.self_attn.q_proj.weight
  - module.model.layers.2.input_layernorm.weightFlat tensor shape: torch.Size([7079936])

  - module.model.layers.1.mlp.down_proj.weightBuffer size: 28319744 bytes

  - module.model.layers.1.mlp.up_proj.weightData type: torch.float32

  - module.model.layers.1.mlp.gate_proj.weight
  - module.model.layers.1.post_attention_layernorm.weight
  - module.model.layers.1.self_attn.o_proj.weightBucket parameter names:

  - module.model.layers.1.self_attn.v_proj.weight  - module.model.layers.2.mlp.up_proj.weight

  - module.model.layers.1.self_attn.k_proj.weight  - module.model.layers.2.mlp.gate_proj.weight

  - module.model.layers.1.self_attn.q_proj.weight  - module.model.layers.2.post_attention_layernorm.weight

  - module.model.layers.1.input_layernorm.weight  - module.model.layers.2.self_attn.o_proj.weight

  - module.model.layers.0.mlp.down_proj.weight  - module.model.layers.2.self_attn.v_proj.weight

  - module.model.layers.0.mlp.up_proj.weight  - module.model.layers.2.self_attn.k_proj.weight

  - module.model.layers.0.mlp.gate_proj.weight  - module.model.layers.2.self_attn.q_proj.weight


  - module.model.layers.2.input_layernorm.weight
  - module.model.layers.1.mlp.down_proj.weight
  - module.model.layers.1.mlp.up_proj.weight
  - module.model.layers.1.mlp.gate_proj.weight
  - module.model.layers.1.post_attention_layernorm.weight
  - module.model.layers.1.self_attn.o_proj.weight
  - module.model.layers.1.self_attn.v_proj.weight
  - module.model.layers.1.self_attn.k_proj.weight
  - module.model.layers.1.self_attn.q_proj.weight
  - module.model.layers.1.input_layernorm.weight
  - module.model.layers.0.mlp.down_proj.weight
  - module.model.layers.0.mlp.up_proj.weight
  - module.model.layers.0.mlp.gate_proj.weight

[HOOK INFO]
Batch: Batch_4, Communication: Communication_16_bucket_4, Rank: rank_0
Bucket index: 4
Flat tensor shape: torch.Size([3933184])
Buffer size: 15732736 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.0.post_attention_layernorm.weight
  - module.model.layers.0.self_attn.o_proj.weight
  - module.model.layers.0.self_attn.v_proj.weight
  - module.model.layers.0.self_attn.k_proj.weight
  - module.model.layers.0.self_attn.q_proj.weight
  - module.model.layers.0.input_layernorm.weight
  - module.model.embed_tokens.weight

[HOOK INFO]
Batch: Batch_4, Communication: Communication_16_bucket_4, Rank: rank_1
Bucket index: 4
Flat tensor shape: torch.Size([3933184])
Buffer size: 15732736 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.0.post_attention_layernorm.weight
  - module.model.layers.0.self_attn.o_proj.weight
  - module.model.layers.0.self_attn.v_proj.weight
  - module.model.layers.0.self_attn.k_proj.weight
  - module.model.layers.0.self_attn.q_proj.weight
  - module.model.layers.0.input_layernorm.weight
  - module.model.embed_tokens.weight

[HOOK INFO]
Batch: Batch_5, Communication: Communication_17_bucket_0, Rank: rank_0
Bucket index: 0
Flat tensor shape: torch.Size([721408])
Buffer size: 2885632 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.norm.weight
  - module.model.layers.7.mlp.down_proj.weight

[HOOK INFO]
Batch: Batch_5, Communication: Communication_17_bucket_0, Rank: rank_1
Bucket index: 0
Flat tensor shape: torch.Size([721408])
Buffer size: 2885632 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.norm.weight
  - module.model.layers.7.mlp.down_proj.weight

[HOOK INFO]
Batch: Batch_5, Communication: Communication_18_bucket_1, Rank: rank_0
Bucket index: 1
Flat tensor shape: torch.Size([7079936])
Buffer size: 28319744 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.7.mlp.up_proj.weight
  - module.model.layers.7.mlp.gate_proj.weight
  - module.model.layers.7.post_attention_layernorm.weight
  - module.model.layers.7.self_attn.o_proj.weight
  - module.model.layers.7.self_attn.v_proj.weight
  - module.model.layers.7.self_attn.k_proj.weight
  - module.model.layers.7.self_attn.q_proj.weight
  - module.model.layers.7.input_layernorm.weight
  - module.model.layers.6.mlp.down_proj.weight
  - module.model.layers.6.mlp.up_proj.weight
  - module.model.layers.6.mlp.gate_proj.weight
  - module.model.layers.6.post_attention_layernorm.weight
  - module.model.layers.6.self_attn.o_proj.weight
  - module.model.layers.6.self_attn.v_proj.weight
  - module.model.layers.6.self_attn.k_proj.weight[HOOK INFO]

  - module.model.layers.6.self_attn.q_proj.weightBatch: Batch_5, Communication: Communication_18_bucket_1, Rank: rank_1

  - module.model.layers.6.input_layernorm.weightBucket index: 1

  - module.model.layers.5.mlp.down_proj.weight
  - module.model.layers.5.mlp.up_proj.weight
  - module.model.layers.5.mlp.gate_proj.weight
Flat tensor shape: torch.Size([7079936])

Buffer size: 28319744 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.7.mlp.up_proj.weight
  - module.model.layers.7.mlp.gate_proj.weight
  - module.model.layers.7.post_attention_layernorm.weight
  - module.model.layers.7.self_attn.o_proj.weight
  - module.model.layers.7.self_attn.v_proj.weight
  - module.model.layers.7.self_attn.k_proj.weight
  - module.model.layers.7.self_attn.q_proj.weight
  - module.model.layers.7.input_layernorm.weight
  - module.model.layers.6.mlp.down_proj.weight
  - module.model.layers.6.mlp.up_proj.weight
  - module.model.layers.6.mlp.gate_proj.weight
  - module.model.layers.6.post_attention_layernorm.weight
  - module.model.layers.6.self_attn.o_proj.weight
  - module.model.layers.6.self_attn.v_proj.weight
  - module.model.layers.6.self_attn.k_proj.weight
  - module.model.layers.6.self_attn.q_proj.weight
  - module.model.layers.6.input_layernorm.weight
  - module.model.layers.5.mlp.down_proj.weight
  - module.model.layers.5.mlp.up_proj.weight
  - module.model.layers.5.mlp.gate_proj.weight

[HOOK INFO]
Batch: Batch_5, Communication: Communication_19_bucket_2, Rank: rank_1
Bucket index: 2
Flat tensor shape: torch.Size([7015424])[HOOK INFO]

Buffer size: 28061696 bytesBatch: Batch_5, Communication: Communication_19_bucket_2, Rank: rank_0

Data type: torch.float32Bucket index: 2

Bucket parameter names:Flat tensor shape: torch.Size([7015424])

  - module.model.layers.5.post_attention_layernorm.weightBuffer size: 28061696 bytes

  - module.model.layers.5.self_attn.o_proj.weightData type: torch.float32

  - module.model.layers.5.self_attn.v_proj.weight
  - module.model.layers.5.self_attn.k_proj.weight
  - module.model.layers.5.self_attn.q_proj.weight
Bucket parameter names:  - module.model.layers.5.input_layernorm.weight

  - module.model.layers.5.post_attention_layernorm.weight  - module.model.layers.4.mlp.down_proj.weight

  - module.model.layers.5.self_attn.o_proj.weight  - module.model.layers.4.mlp.up_proj.weight

  - module.model.layers.5.self_attn.v_proj.weight  - module.model.layers.4.mlp.gate_proj.weight

  - module.model.layers.5.self_attn.k_proj.weight  - module.model.layers.4.post_attention_layernorm.weight

  - module.model.layers.5.self_attn.q_proj.weight  - module.model.layers.4.self_attn.o_proj.weight

  - module.model.layers.5.input_layernorm.weight  - module.model.layers.4.self_attn.v_proj.weight

  - module.model.layers.4.mlp.down_proj.weight  - module.model.layers.4.self_attn.k_proj.weight

  - module.model.layers.4.mlp.up_proj.weight  - module.model.layers.4.self_attn.q_proj.weight

  - module.model.layers.4.mlp.gate_proj.weight  - module.model.layers.4.input_layernorm.weight

  - module.model.layers.4.post_attention_layernorm.weight  - module.model.layers.3.mlp.down_proj.weight

  - module.model.layers.4.self_attn.o_proj.weight  - module.model.layers.3.mlp.up_proj.weight

  - module.model.layers.4.self_attn.v_proj.weight  - module.model.layers.3.mlp.gate_proj.weight

  - module.model.layers.4.self_attn.k_proj.weight  - module.model.layers.3.post_attention_layernorm.weight

  - module.model.layers.4.self_attn.q_proj.weight  - module.model.layers.3.self_attn.o_proj.weight

  - module.model.layers.4.input_layernorm.weight  - module.model.layers.3.self_attn.v_proj.weight

  - module.model.layers.3.mlp.down_proj.weight  - module.model.layers.3.self_attn.k_proj.weight

  - module.model.layers.3.mlp.up_proj.weight  - module.model.layers.3.self_attn.q_proj.weight

  - module.model.layers.3.mlp.gate_proj.weight  - module.model.layers.3.input_layernorm.weight

  - module.model.layers.3.post_attention_layernorm.weight  - module.model.layers.2.mlp.down_proj.weight

  - module.model.layers.3.self_attn.o_proj.weight

  - module.model.layers.3.self_attn.v_proj.weight
  - module.model.layers.3.self_attn.k_proj.weight
  - module.model.layers.3.self_attn.q_proj.weight
  - module.model.layers.3.input_layernorm.weight
  - module.model.layers.2.mlp.down_proj.weight

[HOOK INFO]
Batch: Batch_5, Communication: Communication_20_bucket_3, Rank: rank_0
Bucket index: 3
Flat tensor shape: torch.Size([7079936])
Buffer size: 28319744 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.2.mlp.up_proj.weight
  - module.model.layers.2.mlp.gate_proj.weight
  - module.model.layers.2.post_attention_layernorm.weight
  - module.model.layers.2.self_attn.o_proj.weight
  - module.model.layers.2.self_attn.v_proj.weight
  - module.model.layers.2.self_attn.k_proj.weight
  - module.model.layers.2.self_attn.q_proj.weight
  - module.model.layers.2.input_layernorm.weight
  - module.model.layers.1.mlp.down_proj.weight
  - module.model.layers.1.mlp.up_proj.weight
  - module.model.layers.1.mlp.gate_proj.weight
  - module.model.layers.1.post_attention_layernorm.weight
  - module.model.layers.1.self_attn.o_proj.weight
  - module.model.layers.1.self_attn.v_proj.weight
  - module.model.layers.1.self_attn.k_proj.weight
  - module.model.layers.1.self_attn.q_proj.weight
  - module.model.layers.1.input_layernorm.weight
  - module.model.layers.0.mlp.down_proj.weight
  - module.model.layers.0.mlp.up_proj.weight
  - module.model.layers.0.mlp.gate_proj.weight

[HOOK INFO]
Batch: Batch_5, Communication: Communication_20_bucket_3, Rank: rank_1
Bucket index: 3
Flat tensor shape: torch.Size([7079936])
Buffer size: 28319744 bytes
Data type: torch.float32
Bucket parameter names:
  - module.model.layers.2.mlp.up_proj.weight
  - module.model.layers.2.mlp.gate_proj.weight
  - module.model.layers.2.post_attention_layernorm.weight
  - module.model.layers.2.self_attn.o_proj.weight
  - module.model.layers.2.self_attn.v_proj.weight
  - module.model.layers.2.self_attn.k_proj.weight
  - module.model.layers.2.self_attn.q_proj.weight
  - module.model.layers.2.input_layernorm.weight
  - module.model.layers.1.mlp.down_proj.weight
  - module.model.layers.1.mlp.up_proj.weight
  - module.model.layers.1.mlp.gate_proj.weight
  - module.model.layers.1.post_attention_layernorm.weight
  - module.model.layers.1.self_attn.o_proj.weight
  - module.model.layers.1.self_attn.v_proj.weight
  - module.model.layers.1.self_attn.k_proj.weight
  - module.model.layers.1.self_attn.q_proj.weight
  - module.model.layers.1.input_layernorm.weight
  - module.model.layers.0.mlp.down_proj.weight
  - module.model.layers.0.mlp.up_proj.weight
  - module.model.layers.0.mlp.gate_proj.weight