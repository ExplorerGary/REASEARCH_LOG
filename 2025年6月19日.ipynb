{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb96b452",
   "metadata": {},
   "source": [
    "# 2025年6月19日\n",
    "\n",
    "针对问题的回答：\n",
    "1. 再重复一下 gamma 是ggd的shape parameter?\n",
    "\n",
    "    是的。 现在算出来的Gamma是指：\n",
    "    \n",
    "        由bucket中存下的各个tensor展平后，粘起来的一个一维向量。\n",
    "        \n",
    "        使用MLE拟合出来的GGD中的shape parameter。\n",
    "    \n",
    "        跟论文中的γ是一样的。\n",
    "\n",
    "2. 所谓的异常情况的数据是什么？ weights? bias 的gradients?\n",
    "    异常情况是指我在昨天（2025年6月18日）进行抽样观测中发现：\n",
    "    \n",
    "        minimind大模型的在执行分布式训练时\n",
    "        \n",
    "        传输的部分bucket中。gradient，几乎全部为零的情况。\n",
    "\n",
    "        这种事情发生的概率不低，约为1/5左右。但由于保存逻辑的错误，我并没有记录发生这种问题的具体bucket。\n",
    "\n",
    "        如图所示。\n",
    "\n",
    "![异常情况示意图](img/2025年6月19日_1.png)\n",
    "\n",
    "\n",
    "        这会导致：matlab MLE拟合超过最大iter限制。而无法拟合，此时，会观察到Gamma约为0.1左右。\n",
    "\n",
    "        同时观察到这里的Beta(scale parameter)很大概率是0。\n",
    "\n",
    "\n",
    "\n",
    "今天干的事：\n",
    "1. GGD Fitting：\n",
    "    虽然昨天已经挂起了批处理程序，但是计算的耗时还是超乎我的预料。\n",
    "    遂更换64核节点加快计算。\n",
    "\n",
    "    关于Gemma：\n",
    "        截止到2025年6月19日17:43:13\n",
    "\n",
    "![Gamma数据](img/2025年6月19日_2.png)\n",
    "\n",
    "\n",
    "        统计数据：\n",
    "            Statistics for 'Gamma' column:\n",
    "            count    5540.000000\n",
    "            mean        0.916227\n",
    "            std         0.467576\n",
    "            min         0.021561\n",
    "            25%         0.764437\n",
    "            50%         1.008272\n",
    "            75%         1.110532\n",
    "            max         1.627668\n",
    "            Name: Gamma, dtype: float64\n",
    "\n",
    "        观察到Gamma集中在三个点：\n",
    "            0.1 [猜测有可能是因为前文提到的“异常情况”导致的，MLE没有正常拟合]\n",
    "            1\n",
    "            略大于1.5\n",
    "\n",
    "\n",
    "        请问我还要继续用全数据拟合GGD，还是选用直方图进行拟合？使用直方图会显著加快我们的进程。\n",
    "\n",
    "    关于entropy：\n",
    "        截止到2025年6月19日17:53:57\n",
    "        \n",
    "![Gamma数据](img/2025年6月19日_3.png)\n",
    "\n",
    "\n",
    "        注：index是按照batch和communication进行划分的。从左到右是刚开始训练到训练结束。\n",
    "\n",
    "\n",
    "        统计数据：\n",
    "            Statistics for 'Entropy' column:\n",
    "            count    220790.000000\n",
    "            mean          3.193310\n",
    "            std           1.265674\n",
    "            min           0.283477\n",
    "            25%           2.101873\n",
    "            50%           3.088205\n",
    "            75%           3.808533\n",
    "            max           6.374693\n",
    "            Name: Entropy, dtype: float64\n",
    "        \n",
    "        观察到：\n",
    "            entropy按照不同的bucket成集中趋势。而且bucket从0到4似乎熵在不断减小。\n",
    "\n",
    "                这可能提示我们：模型在训练过程中对靠近输出端的层可能具有更强的关注度，导致熵更大。\n",
    "            \n",
    "            具体的bucket里装了啥由于保存逻辑的错误，暂时无法知晓。但会很快准备就绪。\n",
    "\n",
    "            rank 0 和 rank 1的分布相差无几。\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. 快速启动大模型微调：\n",
    "    模型部署（完成）\n",
    "    \n",
    "    下一步：\n",
    "        研究HuggingFace下怎么挂上DDP钩子\n",
    "        按照教程启动微调\n",
    "\n",
    "3. 访问llama的大模型\n",
    "    llama2\n",
    "    llama3\n",
    "    llama3.1\n",
    "    llama3.2\n",
    "    llama4\n",
    "\n",
    "    均可以访问\n",
    "\n",
    "\n",
    "\n",
    "注：我明天（2025年6月20日）有对我重要的事，请假一天。\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
